# ingest.py
import os
from llama_index.core import SimpleDirectoryReader

import logging
from typing import List
import pinecone

from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.vector_stores.pinecone import PineconeVectorStore
from llama_index.core import StorageContext, VectorStoreIndex
from typing import List
from llama_index.core.schema import BaseNode

from llama_index.core import Document
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.node_parser import SemanticSplitterNodeParser
from llama_index.embeddings.openai import OpenAIEmbedding

from dotenv import load_dotenv

load_dotenv()

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = os.getenv("PINECONE_INDEX_NAME")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] - %(message)s"
)
logger = logging.getLogger(__name__)


def ingest_pdfs(data_dir: str = "./data"):
    """
    ƒê·ªçc t·∫•t c·∫£ file PDF trong th∆∞ m·ª•c data/ 
    v√† tr·∫£ v·ªÅ list Document objects
    """
    if not os.path.exists(data_dir):
        raise ValueError(f"‚ùå Th∆∞ m·ª•c {data_dir} kh√¥ng t·ªìn t·∫°i!")

    # SimpleDirectoryReader m·∫∑c ƒë·ªãnh s·ª≠ d·ª•ng pypdf ƒë·ªÉ ƒë·ªçc file PDF
    reader = SimpleDirectoryReader(
        input_dir=data_dir,
        required_exts=[".pdf"],   # ch·ªâ ƒë·ªçc file PDF
        recursive=True            # ƒë·ªçc c·∫£ sub-folder n·∫øu c√≥
    )

    docs = reader.load_data()
    print(f"‚úÖ ƒê√£ ingest {len(docs)} file PDF t·ª´ th∆∞ m·ª•c {data_dir}")
    return docs

def chunk_documents(docs: List[Document], use_semantic: bool = True):
    """
    Chunking d·ªØ li·ªáu v·ªõi SentenceSplitter + (t√πy ch·ªçn) SemanticSplitterNodeParser
    """

    if not docs:
        logger.warning("‚ùå Kh√¥ng c√≥ document n√†o ƒë·ªÉ chunking.")
        return []

    # Step 1: Sentence-based chunking
    logger.info("üëâ B·∫Øt ƒë·∫ßu chunking b·∫±ng SentenceSplitter...")
    sentence_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=50)
    sentence_nodes = sentence_splitter.get_nodes_from_documents(docs)
    logger.info(f"‚úÖ SentenceSplitter t·∫°o ra {len(sentence_nodes)} chunks.")

    if not use_semantic:
        return sentence_nodes

    # Step 2: Semantic-based chunking
    logger.info("üëâ B·∫Øt ƒë·∫ßu chunking b·∫±ng SemanticSplitterNodeParser...")
    embed_model = OpenAIEmbedding(model="text-embedding-3-small")
    semantic_splitter = SemanticSplitterNodeParser(
        embed_model=embed_model,
        buffer_size=1,
        breakpoint_percentile_threshold=95,
    )

    semantic_nodes = semantic_splitter.get_nodes_from_documents(docs)
    logger.info(f"‚úÖ SemanticSplitter t·∫°o ra {len(semantic_nodes)} chunks.")

    return semantic_nodes
def push_to_pinecone(nodes: List[BaseNode]):
    """
    T·∫°o embeddings cho c√°c nodes v√† l∆∞u v√†o Pinecone index 'ragflow'
    """
    if not nodes:
        logger.warning("‚ùå Kh√¥ng c√≥ nodes n√†o ƒë·ªÉ embed.")
        return None
    pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)
    if INDEX_NAME not in pc.list_indexes().names():
    # N·∫øu index ch∆∞a t·ªìn t·∫°i ‚Üí t·∫°o m·ªõi
        logger.info(f"üëâ Ch∆∞a c√≥ index '{INDEX_NAME}', t·∫°o m·ªõi...")
        pc.create_index(
            INDEX_NAME,
            dimension=1536,  # text-embedding-3-small c√≥ dimension 1536
            metric="cosine"
        )

    pinecone_index = pc.Index(INDEX_NAME)

    # Embedding model
    embed_model = OpenAIEmbedding(model="text-embedding-3-small")

    # T·∫°o VectorStore k·∫øt n·ªëi v·ªõi Pinecone
    vector_store = PineconeVectorStore(pinecone_index)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # Build index v√† push l√™n Pinecone
    logger.info("üëâ ƒêang t·∫°o VectorStoreIndex v√† ƒë·∫©y embeddings v√†o Pinecone...")
    index = VectorStoreIndex(
        nodes,
        storage_context=storage_context,
        embed_model=embed_model
    )

    logger.info(f"‚úÖ ƒê√£ push {len(nodes)} chunks v√†o Pinecone index '{INDEX_NAME}'.")
    return index

if __name__ == "__main__":
    docs = ingest_pdfs(data_dir="./data")
    nodes = chunk_documents(docs, use_semantic=True)
    push_to_pinecone(nodes)
    logger.info(f"T·ªïng s·ªë nodes sau chunking: {len(nodes)}")
    if nodes:
        logger.info("üìÑ M·∫´u chunk:")
        logger.info(nodes[0].get_content()[:500])   