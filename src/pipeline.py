import os
import logging
import glob
from dotenv import load_dotenv
from extract import extract_text_with_fallback, extract_tables_from_pdf
from transform import split_chunk_semantic_sentence
from load import upsert_chunks_to_pinecone

load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = "ragflow"
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] - %(message)s")
logger = logging.getLogger(__name__)

def get_all_pdf_files_in_data():
	"""L·∫•y t·∫•t c·∫£ file PDF trong th∆∞ m·ª•c data"""
	data_dir = os.path.join(os.path.dirname(__file__), '..', 'data')
	pdf_files = glob.glob(os.path.join(data_dir, "*.pdf"))
	logger.info(f"üìÅ T√¨m th·∫•y {len(pdf_files)} file PDF trong th∆∞ m·ª•c data")
	return pdf_files

# ----------- ETL Pipeline -----------
def pipeline_etl(pdf_paths: list = None, output_csv: str = "./output/tables", max_tokens: int = 1024, namespace: str = "default"):
	"""
	ETL Pipeline x·ª≠ l√Ω nhi·ªÅu PDF files
	Args:
		pdf_paths: Danh s√°ch ƒë∆∞·ªùng d·∫´n PDF files. N·∫øu None, s·∫Ω l·∫•y t·∫•t c·∫£ PDF trong data/
		output_csv: ƒê∆∞·ªùng d·∫´n output CSV
		max_tokens: S·ªë token t·ªëi ƒëa cho m·ªói chunk
		namespace: Namespace trong Pinecone
	"""
	# N·∫øu kh√¥ng c√≥ pdf_paths, l·∫•y t·∫•t c·∫£ PDF trong data/
	if pdf_paths is None:
		pdf_paths = get_all_pdf_files_in_data()
	
	if not pdf_paths:
		logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF n√†o ƒë·ªÉ x·ª≠ l√Ω!")
		return
	
	all_chunks = []
	
	for pdf_path in pdf_paths:
		logger.info(f"üîÑ X·ª≠ l√Ω file: {pdf_path}")
		try:
			# Extract text v·ªõi output file
			pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]
			text_output = f"./output/{pdf_name}_extracted_text.txt"
			docs = extract_text_with_fallback(pdf_path, output_txt=text_output)
			
			# Extract tables
			tables = extract_tables_from_pdf(pdf_path, output_csv=f"{output_csv}_{os.path.basename(pdf_path)}")
			if tables:
				docs[0]["tables"] = [tables[0].head().to_dict()]
			
			# Transform
			chunks = split_chunk_semantic_sentence(docs, max_tokens=max_tokens, openai_api_key=OPENAI_API_KEY)
			
			# Th√™m metadata file cho m·ªói chunk
			for chunk in chunks:
				chunk["source_file"] = os.path.basename(pdf_path)
			
			all_chunks.extend(chunks)
			logger.info(f"‚úÖ X·ª≠ l√Ω xong {pdf_path}: {len(chunks)} chunks")
			
		except Exception as e:
			logger.error(f"‚ùå L·ªói x·ª≠ l√Ω file {pdf_path}: {e}")
	
	# Load t·∫•t c·∫£ chunks v√†o Pinecone
	if all_chunks:
		logger.info(f"üîº ƒêang upload {len(all_chunks)} chunks v√†o Pinecone...")
		upsert_chunks_to_pinecone(
			all_chunks,
			index_name=INDEX_NAME,
			openai_api_key=OPENAI_API_KEY,
			pinecone_api_key=PINECONE_API_KEY,
			namespace=namespace,
		)
		logger.info("‚úÖ Pipeline ETL ho√†n t·∫•t.")
	else:
		logger.warning("‚ö†Ô∏è Kh√¥ng c√≥ chunks n√†o ƒë·ªÉ upload!")

if __name__ == "__main__":
	# Ch·∫°y ETL v·ªõi t·∫•t c·∫£ PDF c√≥ trong th∆∞ m·ª•c data/
	logger.info("üöÄ B·∫Øt ƒë·∫ßu ETL Pipeline v·ªõi t·∫•t c·∫£ PDF trong data/")
	
	# T·ª± ƒë·ªông ph√°t hi·ªán v√† x·ª≠ l√Ω t·∫•t c·∫£ file PDF trong data/
	pdf_files = get_all_pdf_files_in_data()
	
	if pdf_files:
		logger.info(f"üìÅ T√¨m th·∫•y {len(pdf_files)} file PDF:")
		for pdf_file in pdf_files:
			logger.info(f"   - {os.path.basename(pdf_file)}")
		
		pipeline_etl(pdf_paths=pdf_files, output_csv="./output/tables", max_tokens=512)
	else:
		logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF n√†o trong th∆∞ m·ª•c data/")
		logger.info("üí° H√£y ƒë·∫∑t file PDF v√†o th∆∞ m·ª•c data/ ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω")